{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T16:58:30.085562Z",
     "start_time": "2021-09-04T16:58:25.398139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71583, 43) (7942, 43) (71583, 3) (7942, 3)\n"
     ]
    }
   ],
   "source": [
    "########################start##############################\n",
    "import pandas as pd\n",
    "df=pd.read_csv('./datas/Data_new.csv',index_col=0)\n",
    "df.index=pd.to_datetime(df.index,format='%Y-%m-%d') \n",
    "df_train=df.loc['2020-01-07':'2021-04-02'] \n",
    "df_test=df.loc['2021-04-02':'2021-04-23']\n",
    "\n",
    "name=['cityin','cityto','incity','city']\n",
    "name1=['cityin','cityto','incity']\n",
    "x_train=df_train.drop(name,axis=1).values  \n",
    "y_train=df_train[name1].values\n",
    "x_test=df_test.drop(name,axis=1).values  \n",
    "y_test=df_test[name1].values\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "#####################标准化##############################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(x_train)\n",
    "x_train = ss.transform(x_train) \n",
    "x_test = ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T16:58:32.554092Z",
     "start_time": "2021-09-04T16:58:31.720691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  There is this folder!  ---\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 中文字体设置-黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "sns.set(font='SimHei',font_scale=1.5)  # 解决Seaborn中文显示问题并调整字体大小\n",
    "sns.set_style(\"white\")\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:                   \n",
    "        os.makedirs(path)            \n",
    "        print(\"---  new folder...  ---\")\n",
    "        print(\"---  OK  ---\")\n",
    "    else:\n",
    "        print(\"---  There is this folder!  ---\")\n",
    "mkdir(\"./results/\")        \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,median_absolute_error,r2_score,mean_squared_log_error\n",
    "\n",
    "def calculate(y_true, y_predict, n, p):\n",
    "    y_true = y_true\n",
    "    y_predict = y_predict\n",
    "    mse = mean_squared_error(y_true, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_predict))\n",
    "    mae = mean_absolute_error(y_true, y_predict)\n",
    "    r2 = r2_score(y_true, y_predict)\n",
    "    mad = median_absolute_error(y_true, y_predict)\n",
    "    mape = np.mean(np.abs((y_true - y_predict) / y_true)) * 100\n",
    "    r2_adjusted = 1-((1-r2)*(n-1))/(n-p-1)\n",
    "    # print('MSE: ', mse)\n",
    "    # print('RMSE: ', rmse)\n",
    "    # print('MAE: ', mae)\n",
    "    # print('R2: ', r2)\n",
    "    # print('MAD:', mad)\n",
    "    # print('MAPE:', mape)\n",
    "    # print('R2_Adjusted: ',r2_adjusted)\n",
    "    return mse,rmse,mae,r2,mad,mape,r2_adjusted\n",
    "# 将参数和评估结果写入文件\n",
    "def write_csv_result(path_1,path_2,all_metrics,all_parameter):\n",
    "    with open(path_1,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "        f = csv.writer(f)\n",
    "        f.writerow(all_metrics)\n",
    "    with open(path_2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "        f = csv.writer(f)\n",
    "        f.writerow(all_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T17:01:18.694761Z",
     "start_time": "2021-09-04T17:00:53.379282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start....1/480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_estimators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ff159370a2ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Migration cityin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m \u001b[0madjust_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-ff159370a2ed>\u001b[0m in \u001b[0;36madjust_mlp\u001b[1;34m(train_x, test_x, train_y, test_y, name)\u001b[0m\n\u001b[0;32m    108\u001b[0m                         \u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmae\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr2_adjusted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmse1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmse2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmse3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrmse2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrmse3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmae1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmae2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmae3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr21\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr22\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr23\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmad1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmad2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmad3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmape2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmape3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_adjusted1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr2_adjusted2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr2_adjusted3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                         \u001b[0mall_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmae\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr2_adjusted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                         \u001b[0mall_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                         \u001b[1;31m# print(all_m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mwrite_csv_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath_p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_m\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_estimators' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "from sklearn.multioutput import RegressorChain\n",
    "import csv\n",
    "def adjust_mlp(train_x,test_x,train_y,test_y,name):\n",
    "    path_a = \"./results/\" + \"Ada_\" + name + \"_\" + \"assess.csv\"\n",
    "    path_p = \"./results/\" + \"Ada_\" + name + \"_\" + \"parameter.csv\"\n",
    "    path_b = \"./results/\" + \"Ada_\" + name + \"_\" + \"best.csv\"\n",
    "    model_path = \"./results/\" + \"Ada_\" + name + \"_\" + \"best.m\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    max_iter = [5000,10000,15000,20000]\n",
    "    tol = [1e-3,2e-3,1e-4,1e-2]\n",
    "    learning_rate_init = [1e-2,1e-3,1e-4]\n",
    "    hidden_layer_sizes = list(combinations([64,32,16,8,4], 3))\n",
    "    all_nb = len(max_iter) * len(tol) * len(learning_rate_init) * len(hidden_layer_sizes)\n",
    "    num = 1\n",
    "    # 用于重启训练模型，提高效率，不重复跑相同的实验\n",
    "    if(os.path.exists(path_a)):\n",
    "        data = pd.read_csv(path_a)\n",
    "        if(data.shape[0]>1):\n",
    "            try:\n",
    "                nums = int(data.values[-1,0])\n",
    "            except:\n",
    "                nums=0\n",
    "        else:\n",
    "            nums = 0\n",
    "    else:\n",
    "        nums = 0\n",
    "        col_a = ['num','mse','rmse','mae','r2','mad','mape','r2_adjusted']\n",
    "        col_p = ['num','max_iter','tol','learning_rate_init','hidden_layer_sizes']\n",
    "        write_csv_result(path_a,path_p,col_a,col_p)\n",
    "    # 用于保存最好的模型\n",
    "    if(os.path.exists(path_b)):\n",
    "        # best_results = pd.read_csv(path_b)\n",
    "        # if(best_results.shape[0]>1):\n",
    "        #     best_result = best_results[\"rmse\"].values[-1]\n",
    "        # else:\n",
    "        best_result = 10*10**30\n",
    "    else:\n",
    "        with open(path_b,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "            f = csv.writer(f)\n",
    "            f.writerow(['num','mse','rmse','mae','r2','mad','mape','r2_adjusted'])\n",
    "        best_result = 10*10**30\n",
    "\n",
    "    for m in max_iter:\n",
    "        for t in tol:\n",
    "            for l in learning_rate_init:\n",
    "                for hd in hidden_layer_sizes:\n",
    "                    if(nums>=num):\n",
    "                        num = num+1\n",
    "                    else:\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        mlp = MLPRegressor(hidden_layer_sizes=hd, activation=\"relu\",\n",
    "                                         solver='adam', alpha=0.0001,\n",
    "                                         batch_size='auto', learning_rate=\"constant\",\n",
    "                                         learning_rate_init=l,\n",
    "                                         power_t=0.5, max_iter=m,tol=t)\n",
    "                        wrapper = RegressorChain(mlp)\n",
    "                        wrapper.fit(train_x,train_y)\n",
    "                        pred_test = wrapper.predict(test_x)\n",
    "\n",
    "                        y_test=test_y       \n",
    "                        y_test1=[]\n",
    "                        for i in y_test:\n",
    "                            y_test1.append(i[0])\n",
    "                        pred_test1=[]\n",
    "                        for i in pred_test:\n",
    "                            pred_test1.append(i[0])\n",
    "                        y_test2=[]\n",
    "                        for i in y_test:\n",
    "                            y_test2.append(i[1])\n",
    "                        pred_test2=[]\n",
    "                        for i in pred_test:\n",
    "                            pred_test2.append(i[1])\n",
    "                        y_test3=[]\n",
    "                        for i in y_test:\n",
    "                            y_test3.append(i[2])\n",
    "                        pred_test3=[]\n",
    "                        for i in pred_test:\n",
    "                            pred_test3.append(i[2])\n",
    "                        pred_test1 = np.array(pred_test1).reshape(-1,1)\n",
    "                        y_test1 = np.array(y_test1).reshape(-1,1)\n",
    "                        sample_n = pred_test1.shape[0]\n",
    "                        feature_n = x_test.shape[1]\n",
    "                        mse1,rmse1,mae1,r21,mad1,mape1,r2_adjusted1 = calculate(y_test1,pred_test1,sample_n,feature_n)\n",
    "\n",
    "\n",
    "                        pred_test2 = np.array(pred_test2).reshape(-1,1)\n",
    "                        y_test2 = np.array(y_test2).reshape(-1,1)\n",
    "                        sample_n = pred_test2.shape[0]\n",
    "                        feature_n = x_test.shape[1]\n",
    "                        mse2,rmse2,mae2,r22,mad2,mape2,r2_adjusted2 = calculate(y_test2,pred_test2,sample_n,feature_n)\n",
    "\n",
    "\n",
    "                        pred_test3 = np.array(pred_test3).reshape(-1,1)\n",
    "                        y_test3 = np.array(y_test3).reshape(-1,1)\n",
    "                        sample_n = pred_test3.shape[0]\n",
    "                        feature_n = x_test.shape[1]\n",
    "                        mse3,rmse3,mae3,r23,mad3,mape3,r2_adjusted3 = calculate(y_test3,pred_test3,sample_n,feature_n)\n",
    "\n",
    "\n",
    "                        pred_test4 = pred_test.reshape(-1,1)\n",
    "                        y_test4 = y_test.reshape(-1,1)\n",
    "                        sample_n = pred_test4.shape[0]\n",
    "                        feature_n = x_test.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = (mse1+mse2+mse3)/3,(rmse1+rmse2+rmse3)/3,(mae1+mae2+mae3)/3,(r21+r22+r23)/3,(mad1+mad2+mad3)/3,(mape1+mape2+mape3)/3,(r2_adjusted1+r2_adjusted2+r2_adjusted3)/3\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,m,t,l,hd]\n",
    "                        # print(all_m)\n",
    "                        write_csv_result(path_a,path_p,all_m,all_p)  \n",
    "\n",
    "                        if(rmse < best_result):\n",
    "                            joblib.dump(filename=model_path,value=mlp,compress=True)\n",
    "                            # joblib.dump(model,model_path)\n",
    "                            with open(path_b,\"w\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow([num,mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "                                f.writerow([num,mse1,rmse1,mae1,r21,mad1,mape1,r2_adjusted1])\n",
    "                                f.writerow([num,mse2,rmse2,mae2,r22,mad2,mape2,r2_adjusted2])\n",
    "                                f.writerow([num,mse3,rmse3,mae3,r23,mad3,mape3,r2_adjusted3])\n",
    "                            best_result = rmse\n",
    "                            print(\"current best result, mse:{},mae:{},rmse:{},mad:{},r2:{},mape:{},r2 adjusted:{}\".format(mse,mad,rmse,mad,r2,mape,r2_adjusted))\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\")  \n",
    "\n",
    "name='Migration cityin'\n",
    "adjust_mlp(x_train,x_test,y_train,y_test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
